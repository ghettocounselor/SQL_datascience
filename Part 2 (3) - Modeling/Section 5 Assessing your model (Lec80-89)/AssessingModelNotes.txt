
Lecture 81 Accuracy Paradox https://www.udemy.com/datascience/learn/lecture/3498462
It's the idea that Accuracy is the best way to see if a model is working. 
Answer is it's not. 

Lecture 82 Cumulative Accuracy Profile (CAP)
https://en.wikipedia.org/wiki/Cumulative_accuracy_profile
	The cumulative accuracy profile (CAP) is used in data science to visualize the discriminative 
	power of a model. The CAP of a model represents the cumulative number of positive outcomes along 
	the y-axis versus the corresponding cumulative number of a classifying parameter along the x-axis. 
	The CAP is distinct from the receiver operating characteristic (ROC), which plots the true-positive 
	rate against the false-positive rate.

Is different from a ROC = Reciever Operating Characteristic (ROC)
https://en.wikipedia.org/wiki/Receiver_operating_characteristic
These are curves expressing a distribution of binary success. 
	A receiver operating characteristic curve, or ROC curve, is a graphical plot that illustrates 
	the diagnostic ability of a binary classifier system as its discrimination threshold is varied.

	The ROC curve is created by plotting the true positive rate (TPR) against the false positive 
	rate (FPR) at various threshold settings. The true-positive rate is also known as sensitivity, 
	recall or probability of detection[4] in machine learning. The false-positive rate is also known 
	as the fall-out or probability of false alarm[4] and can be calculated as (1 − specificity). It 
	can also be thought of as a plot of the power as a function of the Type I Error of the decision 
	rule (when the performance is calculated from just a sample of the population, it can be thought 
	of as estimators of these quantities). The ROC curve is thus the sensitivity as a function of fall-out. 
	In general, if the probability distributions for both detection and false alarm are known, the ROC 
	curve can be generated by plotting the cumulative distribution function (area under the probability 
	distribution from {\displaystyle -\infty } -\infty  to the discrimination threshold) of the detection 
	probability in the y-axis versus the cumulative distribution function of the false-alarm probability 
	on the x-axis.

Lecture 83 How to build a CAP curve in Excel
Gretl => Model => Limited Dependent Values => Logit => Ordered (!!)

Analysis => Forecasts => Save (blue + sign) => P_Hat (forecast of Exited Probability)

File => Export => Save Rownumber, Exited and P_Hat as a CSV
Open in Excel 
put on filters and filter P_Hat largest to smallest, then take filters off. 

Lecture 84 CAP build and assessment https://www.udemy.com/datascience/learn/lecture/3498468
See CAP_RuleOfThumbAssessment
See the XLS's that are in the folder. 

Lecture 86 How to Test data to prevent overfitting your model
Basic idea; take some of your original data, before building your model and seperate it. 
Build your model with the rest of the original data (training data); this will give you an Accuracy Ratio for Train. 
Then test your model with the separated data (test data); this will give you an Accuracy Ratio for Test.
If you do not get a similar Accuracy Ratio that may mean that your Model is Overfitted. 

Lecture 87 Apply the model to test data
Open both Train and Test sets of data and copy Test into Train except for the Exited column. 

Build model as before;
Model 1: Logit, using observations 1-10000
Dependent variable: Exited
Standard errors based on Hessian

                   coefficient    std. error       z       p-value 
  -----------------------------------------------------------------
  const            −3.91258       0.237164      −16.50    3.84e-061 ***
  CreditScore      −0.000674866   0.000280272    −2.408   0.0160    **
  Age               0.0726550     0.00257451     28.22    3.24e-175 ***
  NumOfProducts    −0.0950198     0.0475374      −1.999   0.0456    **
  IsActiveMember   −1.07578       0.0576458     −18.66    1.01e-077 ***
  Female            0.526721      0.0544591       9.672   3.97e-022 ***
  Germany           0.747595      0.0650515      11.49    1.44e-030 ***
  Tenure           −0.0158791     0.00934627     −1.699   0.0893    *
  Log_Balance       0.0690263     0.0139592       4.945   7.62e-07  ***

Mean dependent var   0.203700   S.D. dependent var   0.402769
McFadden R-squared   0.152787   Adjusted R-squared   0.151006
Log-likelihood      −4282.570   Akaike criterion     8583.141
Schwarz criterion    8648.034   Hannan-Quinn         8605.107

Number of cases 'correctly predicted' = 8127 (81.3%)
f(beta'x) at mean of independent vars = 0.135
Likelihood ratio test: Chi-square(8) = 1544.64 [0.0000]

           Predicted
               0      1
  Actual 0  7687    276
         1  1597    440

Next we want to do an analysis
Gretl will know the start and finish we want, the new rows. 
Number of pre-forecast ... set to zero
This will give us a prediction for the new records with Probability. 
The question becomes how accurate was the model :) 

So we save the variable (blue + sign) as P_Hat_TestData
now export our data; File => Export => only need Rownumber and P_Hat_TestData
Delete the 1st 10k rows with no predictions
open original test data file and copy the exited column in the sheet with the predictions.

Lecture 88 Compare Training performance and test performance
https://www.udemy.com/datascience/learn/lecture/3498476
Next we'll run the new data through the CAP Curve Template. 
Order our data by probability of leaving P_Hat_TestData largest to smallest. 
Copy data into template. 
Result; test data is performing worse
but only to a small degree
we have a small test sample
A model will generally  perform better on the data it was built on

Section Recap
1. Accuracy Paradox
2. We learned the Cumulative Accuracy Profile (CAP) curve and how to read
3. How to build a CAP curve in Excel and used the template
4. How to assess the model via the CAP curve at 50% (rules of thumb)
5. Discussed overfitting of a model against the data used to construct the model
we used Train data to build model and Test data to compare. 
6. Practiced how to verify models via test data
7. Preview of a model deteriorating which is similar to overfitting
         





























{\rtf1}